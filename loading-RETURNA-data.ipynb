{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the basics...\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#used for moving files around and finding folders\n",
    "import os as os\n",
    "\n",
    "#used to output pretty progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppress warning from pandas\n",
    "from warnings import simplefilter \n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensuring certain directories are present;\n",
    "newpath1 = r'RETA_data'\n",
    "newpath2 = r'output' \n",
    "if not os.path.exists(newpath1):\n",
    "    os.makedirs(newpath1)\n",
    "\n",
    "if not os.path.exists(newpath2):\n",
    "    os.makedirs(newpath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_RETA_file(filepath, keyfile = 'RETURN-A_Keyfile.xlsx', crosswalk_path = None):\n",
    "    \n",
    "    #first, read the Key File\n",
    "    key_file = pd.read_excel(keyfile, sheet_name = \"New Variables\")\n",
    "    \n",
    "    #getting a shortened 3-letter month string\n",
    "    key_file['month_short'] = key_file['month'].str.lower().str[:3]\n",
    "\n",
    "    #combining each column name with its month\n",
    "    key_file['name'] = key_file['month_short'] + '||' + key_file['new_names_for_real_this_time']\n",
    "    \n",
    "    \n",
    "    #reading the RETA file as a list of each line in the file;\n",
    "    with open(filepath, 'r') as file:\n",
    "        orig_txt = file.readlines()\n",
    "    \n",
    "    #doing a tiny bit of cleanup;\n",
    "    #  the \\n was retained at the end of each line, so i am removing that\n",
    "    for i in range(len(orig_txt)):\n",
    "        orig_txt[i] = orig_txt[i].replace('\\n','')\n",
    "\n",
    "    #preparing the dataframe to parse the RETURN-A file into\n",
    "    wide_df = pd.DataFrame()\n",
    "\n",
    "    #creating a string column, where each row contains the entire line from the RETURN-A file.\n",
    "    wide_df['origtxt'] = orig_txt\n",
    "    \n",
    "    #iterate through the different columns from the keyfile.\n",
    "    curr_pos = 0 #tracking the position along the lines.\n",
    "    for i in tqdm(range(len(key_file)), 'Collecting Column Information'):\n",
    "\n",
    "        #identify the datatype;\n",
    "        dtype_ind = key_file['Type_Length'].iloc[i][0] #\"N is numeric, A is character\"\n",
    "\n",
    "        #identify how long this entry is.\n",
    "        entry_length = int(key_file['Type_Length'].iloc[i][1:])\n",
    "\n",
    "        #getting the df;\n",
    "        #  this uses the pandas string accessor;\n",
    "        #  makes a new column that is a subset of the 'origtxt' column\n",
    "        #  names it based on the current column name\n",
    "        wide_df[key_file['name'].iloc[i]] = wide_df['origtxt'].str[curr_pos:curr_pos + entry_length]\n",
    "\n",
    "        #converts the column to numeric if they are marked as such\n",
    "        if(dtype_ind == 'N'):\n",
    "            wide_df[key_file['name'].iloc[i]] = pd.to_numeric(wide_df[key_file['name'].iloc[i]], errors = 'coerce')\n",
    "\n",
    "        #updating the current position.\n",
    "        curr_pos += entry_length\n",
    "    \n",
    "    \n",
    "    #dropping the original text column;\n",
    "    wide_df.drop(columns = ['origtxt'], inplace = True)\n",
    "    \n",
    "    #trying to aggregate this stuff;\n",
    "    \n",
    "    #first, get substrings corresponding to the relevant columns\n",
    "    #   gets the last 31 columns in the list\n",
    "    #   this corresponds to the columns for december.\n",
    "    name_bits = pd.Series(key_file['name'][-31:]).copy()\n",
    "\n",
    "    #removing the 'dec||' prefix from the column names that have it, and adding '_2' to these\n",
    "    #   _2 corresponds to the actual offenses.\n",
    "    name_bits.iloc[:-3] = name_bits.iloc[:-3].str[5:-2] + '_2'\n",
    "\n",
    "    #removing the 'dec||' from the column names that have it\n",
    "    name_bits.iloc[-3:] = name_bits.iloc[-3:].str[5:]\n",
    "\n",
    "    #making a second array of names that correspond to the other columns that we want to drop.\n",
    "    drop_bits = pd.concat([name_bits.copy().iloc[:-3].str[:-1] + '1',\n",
    "                           name_bits.copy().iloc[:-3].str[:-1] + '3',\n",
    "                           name_bits.copy().iloc[:-3].str[:-1] + '4'])\n",
    "    \n",
    "    #converting these from pandas Series to arrays of strings.\n",
    "    name_bits = np.array(name_bits)\n",
    "    drop_bits = np.array(drop_bits)\n",
    "    \n",
    "    #for each;\n",
    "    #  find what rows contain that substring\n",
    "    #  add all of those rows together into a new column\n",
    "    #  drop those old columns from the overall dataframe.\n",
    "    for i in tqdm(range(len(name_bits)), 'Aggregating Column Information'):\n",
    "\n",
    "        #find what rows of the dataframe contain this 'name_bit' (something like murder_2)\n",
    "        cols_mask = np.zeros(len(wide_df.columns), dtype = bool)\n",
    "        for j in range(len(cols_mask)):\n",
    "            cols_mask[j] = name_bits[i] in wide_df.columns[j]\n",
    "\n",
    "        #making a dataframe that only contains the relevant columns\n",
    "        cols_of_interest = wide_df.columns[cols_mask]\n",
    "\n",
    "        #converting each of those columns to numeric;\n",
    "        for j in range(len(cols_of_interest)):\n",
    "            wide_df[cols_of_interest[j]] = pd.to_numeric(wide_df[cols_of_interest[j]], errors='coerce')\n",
    "\n",
    "        #now, we want to add all those columns together\n",
    "        wide_df[name_bits[i]] = wide_df[cols_of_interest].sum(axis = 1, skipna=True)\n",
    "\n",
    "        #dropping these aggregated columns from the dataframe.\n",
    "        wide_df.drop(columns = cols_of_interest, inplace = True)\n",
    "\n",
    "        \n",
    "    #dropping extra columns that correspond to other kinds of counts (unfounded arrests, cleared by arrests, arrests under 18)\n",
    "    for i in range(len(drop_bits)):\n",
    "        \n",
    "        #find what rows of the dataframe contain this 'drop_bit' (something like 'murder_3', 'robbery_1')\n",
    "        cols_mask = np.zeros(len(wide_df.columns), dtype = bool)\n",
    "        for j in range(len(cols_mask)):\n",
    "            cols_mask[j] = drop_bits[i] in wide_df.columns[j]\n",
    "\n",
    "        #dropping these identified columns.\n",
    "        wide_df.drop(columns = wide_df.columns[cols_mask], inplace = True)\n",
    "        \n",
    "    \n",
    "    #dropping a set of columns that are otherwise of no use to us.\n",
    "    #   they represent information about the columns we just dropped.\n",
    "    \n",
    "    #identify these columns.\n",
    "    cols_mask = np.zeros(len(wide_df.columns), dtype = bool)\n",
    "    for j in range(len(cols_mask)):\n",
    "        if('card0' in wide_df.columns[j]):\n",
    "            cols_mask[j] = True\n",
    "\n",
    "        if('card2' in wide_df.columns[j]):\n",
    "            cols_mask[j] = True\n",
    "\n",
    "        if('card3' in wide_df.columns[j]):\n",
    "            cols_mask[j] = True\n",
    "\n",
    "    #dropping those columns.\n",
    "    wide_df.drop(columns = wide_df.columns[cols_mask], inplace = True)\n",
    "    \n",
    "    #creating a new months_reported column based on the card1_type column for each month;\n",
    "    \n",
    "    #identifying the columns\n",
    "    cols_mask = np.zeros(len(wide_df.columns), dtype = bool)\n",
    "    for j in range(len(cols_mask)):\n",
    "        if('card1_type' in wide_df.columns[j]):\n",
    "            cols_mask[j] = True\n",
    "\n",
    "    #paring to just these columns\n",
    "    card1_cols = wide_df.columns[cols_mask]\n",
    "\n",
    "    #preparing the new_months_reported column\n",
    "    wide_df['new_months_reported'] = np.zeros(len(wide_df))\n",
    "    \n",
    "    #for each column (month)\n",
    "    for i in range(len(card1_cols)):\n",
    "        \n",
    "        #get a column of the numbers as integers.\n",
    "        temp = pd.to_numeric(wide_df[card1_cols[i]], errors = 'coerce')\n",
    "\n",
    "        #get a column of true/false, representing if that month has data\n",
    "        col_bool = (temp == 5) | (temp == 2)\n",
    "\n",
    "        #convert it to an array\n",
    "        col_bool = np.array(col_bool)\n",
    "        \n",
    "        #this is the tricky part;\n",
    "        #   when you convert a boolean array to an integer, it becomes either 0 (false) or 1 (true)\n",
    "        #   here, I am adding that column to the new_months_reported; true = +1, false = +0.\n",
    "        wide_df['new_months_reported'] = np.array(wide_df['new_months_reported']) + col_bool.astype(int)\n",
    "    \n",
    "\n",
    "    if(crosswalk_path != None):\n",
    "\n",
    "        #reading the crosswalk file.\n",
    "        crosswalk = pd.read_csv(crosswalk_path, sep = '\t', dtype = 'string')\n",
    "\n",
    "        #paring to what we want;\n",
    "        crosswalk = crosswalk.loc[crosswalk.ORI7 != '-1',['ORI7', 'FIPS_ST','FIPS_COUNTY', 'FPLACE']]\n",
    "\n",
    "        #renaming in preparation to merge;\n",
    "        crosswalk.rename(columns = {'ORI7' : 'hea||ori',\n",
    "                                    'FIPS_ST' : 'STATEFP',\n",
    "                                    'FIPS_COUNTY' : 'COUNTYFP',\n",
    "                                    'FPLACE' : 'PLACEFP'}, inplace = True)\n",
    "        #mergin;\n",
    "        wide_df = wide_df.merge(crosswalk, on='hea||ori', how='left')\n",
    "\n",
    "    return(wide_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running\n",
    "In order for this file to run, you will need to download the RETURN-A data from https://cde.ucr.cjis.gov/LATEST/webapp/#, under Documents & Downloads, then Master File Downloads.\n",
    "\n",
    "Once downloaded, they must be placed in the 'RETA_data' folder. They must be renamed, such that the year of the data is at the front, separated by a '_'.\n",
    "(Ex: KCRETA85.DAT --> 1985_KCRETA85.DAT).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you have downloaded the ICPSR crosswalk file, which is used to associate state, county, and place fips codes, specify the path here;\n",
    "#   the ICPSR crosswalk file can be found at: https://www.icpsr.umich.edu/web/ICPSR/studies/35158#\n",
    "crosswalk_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1985_KCRETA85.DAT', '1986_KCRETA86.DAT', '1987_KCRETA87.DAT', '1988_KCRETA88.DAT', '1989_KCRETA89.DAT', '1990_KEN90', '1991_KEN91', '1992_KEN92', '1993_KEN93', '1994_KEN94', '1995_KEN95', '1996_KEN96', '1997_RETAFIX.Y97', '1998_KCRETA98.DAT', '1999_KCRETA99.DAT', '2000_KCRETA00.DAT', '2001_RETA01.y01', '2002_RETA02', '2003_RETA03.DAT', '2004_RETA04.DAT', '2005_RETA05.DAT', '2006_RETA06.DAT', '2007_RETA07.DAT', '2008_RETA08.DAT', '2009_RETA09.DAT', '2010_RETA10.DAT', '2011_RETA11.DAT', '2012_RETA12.DAT', '2013_RETURNA2013.TXT', '2014_RETA14.DAT', '2015_RETA-COMB.txt', '2016_RETA2016.TXT', '2017_RETA_NATIONAL_MASTER_FILE.txt', '2018_reta-2018.txt', '2019_RETA_NATIONAL_MASTER_FILE_STATIC.txt', '2020_RETA_NATIONAL_MASTER_FILE.txt', '2021_RETA_NATIONAL_MASTER_FILE.txt', '2022_RETA_NATIONAL_MASTER_FILE.txt']\n"
     ]
    }
   ],
   "source": [
    "#Listing the identified RETURN-A files.\n",
    "orig_RETA_files = os.listdir('RETA_data')\n",
    "print(orig_RETA_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================  2017  =======================\n",
      "filename: 2017_RETA_NATIONAL_MASTER_FILE.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Column Information: 100%|██████████| 1548/1548 [00:38<00:00, 40.31it/s]\n",
      "Aggregating Column Information: 100%|██████████| 31/31 [00:19<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "#first, read the files in the folder;\n",
    "orig_RETA_files = os.listdir('RETA_data')\n",
    "\n",
    "#for each identified file;\n",
    "for i in range(len(orig_RETA_files)):\n",
    "    \n",
    "    #get this specific filename\n",
    "    RETA_file = orig_RETA_files[i]\n",
    "    \n",
    "    #get the year from the filename;\n",
    "    year = int(RETA_file[:4])\n",
    "    \n",
    "    print('==================  %s  ======================='%year)\n",
    "    print('filename: %s'%RETA_file)\n",
    "    \n",
    "    #using the function\n",
    "    output_df = read_RETA_file('RETA_data/' + RETA_file, crosswalk_path=crosswalk_path)\n",
    "    \n",
    "    #setting the year to the actual, rather than two digits\n",
    "    output_df['hea||year'] = year\n",
    "    \n",
    "    #saving;\n",
    "    output_df.to_csv('output/' + 'RETA' + str(year) + '.csv', index = False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
